{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gomb - Net\n",
    "### Performance test on the WSSe dataset\n",
    "### And analysis of WSSe experimental data\n",
    "\n",
    "Austin Houston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![OpenInColab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "    https://colab.research.google.com/github/ahoust17/Gomb-Net/blob/main/Eval_WSSe_model.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib import cm\n",
    "\n",
    "# colab interactive plots and drive\n",
    "if 'google.colab' in sys.modules:\n",
    "    from  google.colab import drive \n",
    "    from google.colab import output\n",
    "    drive.mount('/content/drive')\n",
    "    output.enable_custom_widget_manager()\n",
    "else:\n",
    "    %matplotlib widget\n",
    "\n",
    "# other imports\n",
    "import scipy.ndimage as ndimage\n",
    "from scipy.ndimage import label, center_of_mass, gaussian_filter, zoom\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.stats import norm, gaussian_kde\n",
    "from skimage.filters import threshold_otsu\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# for cropping function\n",
    "print('installing DataGenSTEM')\n",
    "!pip install ase\n",
    "!git clone https://github.com/ahoust17/DataGenSTEM.git\n",
    "sys.path.append('./DataGenSTEM/DataGenSTEM')\n",
    "import data_generator as dg\n",
    "\n",
    "# for Gomb-Net\n",
    "print('installing Gomb-Net')\n",
    "!git clone https://github.com/ahoust17/Gomb-Net.git\n",
    "sys.path.append('./Gomb-Net/GombNet')\n",
    "from networks import *\n",
    "from loss_func import GombinatorialLoss\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, you need to add the following shared drive to your google drive:\n",
    "*** WARNING: it is a big file.  Check before you download ***\n",
    "\n",
    "\n",
    "https://drive.google.com/drive/folders/1tDF283xry5op3t594oBUlcNLKbjRTV7C?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following cell after the download is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_folder = 'drive/My Drive/Gomb-Net files'\n",
    "print('available files & directories:')\n",
    "!ls '{shared_folder}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can start with the actual code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "images_dir = str(shared_folder + '/WSSe_dataset/images')\n",
    "labels_dir = str(shared_folder + '/WSSe_dataset/labels')\n",
    "train_loader, val_loader, test_loader = get_dataloaders(images_dir, labels_dir, batch_size = 1, val_split=0.2, test_split=0.1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "input_channels = 1\n",
    "num_classes = 6    # number of output classes\n",
    "num_filters = [64, 128, 256, 512, 1024]\n",
    "\n",
    "model = TwoLeggedUnet(input_channels, num_classes, num_filters, dropout = 0.2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss = GombinatorialLoss(group_size = num_classes//2, loss = 'Dice', epsilon=1e-6, class_weights = None, alpha=2)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the number of trainable parameters\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Get the number of trainable parameters\n",
    "num_trainable_params = count_trainable_parameters(model)\n",
    "print(f\"Number of trainable parameters: {num_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = np.load(str(shared_folder + '/Pretrained_models/WSSe_dataset.pthloss_history.npz'))\n",
    "train_loss = loss_history['train_loss_history']\n",
    "val_loss = loss_history['val_loss_history']\n",
    "\n",
    "plt.figure(figsize = (6,4))\n",
    "plt.plot(train_loss, label='training', color = '#1f77b4')\n",
    "plt.plot(val_loss, label='validation', color = '#d62728')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(title='Losses')\n",
    "plt.tight_layout()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = 0\n",
    "\n",
    "test = test_loader.dataset[test_iter][0].unsqueeze(0)\n",
    "gt = test_loader.dataset[test_iter][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/Users/austin/Documents/GitHub/GombNet/trained_models/WSSe_dataset.pth'\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prediction\n",
    "with torch.no_grad():\n",
    "    #test.to(device)\n",
    "    probability = model(test)\n",
    "    prediction = F.sigmoid(probability)#>0.50\n",
    "probability = probability.squeeze().cpu().numpy() \n",
    "prediction = prediction.squeeze().cpu().numpy()\n",
    "\n",
    "threshold = threshold_otsu(prediction)\n",
    "prediction = (prediction > threshold).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(4,4))\n",
    "ax.imshow(test.squeeze().cpu().numpy(), cmap='gray')\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "num_classes\n",
    "fig, axs = plt.subplots(3,num_classes, sharex=True, sharey=True)\n",
    "\n",
    "for i in range(num_classes):\n",
    "    axs[0,i].imshow(gt[i], cmap='gray')\n",
    "\n",
    "for i in range(num_classes):\n",
    "    axs[1,i].imshow(prediction[i], cmap='gray')\n",
    "\n",
    "for i in range(num_classes)[:2]:\n",
    "    axs[2,i].imshow(probability[i], cmap='plasma')\n",
    "for i in range(num_classes)[2:]:\n",
    "    axs[2,i].imshow(probability[i], cmap='viridis')\n",
    "\n",
    "\n",
    "for ax in axs.ravel():\n",
    "    ax.axis('off')\n",
    "\n",
    "axs[0,0].set_ylabel('GrounTruth')\n",
    "axs[1,0].set_ylabel('Prediction')\n",
    "axs[2,0].set_ylabel('Probability')\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwa_total = 0\n",
    "# dice_total = 0\n",
    "# IoU_total = 0\n",
    "# \n",
    "# def iou(pred, gt):\n",
    "#     intersection = np.logical_and(pred, gt).sum()\n",
    "#     union = np.logical_or(pred, gt).sum()\n",
    "#     return intersection / union\n",
    "# \n",
    "# def dice_coefficient(pred, gt):\n",
    "#     intersection = np.logical_and(pred, gt).sum()\n",
    "#     return 2 * intersection / (pred.sum() + gt.sum())\n",
    "# \n",
    "# # Calculate the accuracy\n",
    "# for i in range(len(test_loader)):\n",
    "#     test = test_loader.dataset[i][0].unsqueeze(0)\n",
    "#     gt = test_loader.dataset[i][1].numpy()  # Convert to numpy array\n",
    "#     \n",
    "#     # Switch ground truth layers\n",
    "#     gt_switched = np.flip(gt, axis=0)\n",
    "#     \n",
    "#     with torch.no_grad():\n",
    "#         probability = model(test)\n",
    "#         prediction = torch.sigmoid(probability)  # Use torch.sigmoid instead of F.sigmoid (deprecated)\n",
    "#     \n",
    "#     probability = probability.squeeze().cpu().numpy()\n",
    "#     prediction = prediction.squeeze().cpu().numpy()\n",
    "# \n",
    "#     threshold = threshold_otsu(prediction)\n",
    "#     prediction = (prediction > threshold).astype(float)\n",
    "#     \n",
    "#     # Calculate metrics for original and switched ground truths\n",
    "#     pwa_original = np.sum(prediction == gt) / np.prod(gt.shape)\n",
    "#     pwa_switched = np.sum(prediction == gt_switched) / np.prod(gt_switched.shape)\n",
    "#     \n",
    "#     dice_original = dice_coefficient(prediction, gt)\n",
    "#     dice_switched = dice_coefficient(prediction, gt_switched)\n",
    "#     \n",
    "#     iou_original = iou(prediction, gt)\n",
    "#     iou_switched = iou(prediction, gt_switched)\n",
    "#     \n",
    "#     # Take the highest value for each metric\n",
    "#     pwa_total += max(pwa_original, pwa_switched)\n",
    "#     dice_total += max(dice_original, dice_switched)\n",
    "#     IoU_total += max(iou_original, iou_switched)\n",
    "# \n",
    "# # Calculate the average for each metric\n",
    "# pwa_total /= len(test_loader)\n",
    "# dice_total /= len(test_loader)\n",
    "# IoU_total /= len(test_loader)\n",
    "# \n",
    "# print(f\"Pixel-wise Accuracy: {pwa_total}\")\n",
    "# print(f\"Mean Dice Coefficient: {dice_total}\")\n",
    "# print(f\"Mean IoU: {IoU_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, on Experimental data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, some useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_intensity(image, atoms, radius=5, zoom_factor=2, norm_size=None):\n",
    "    def circular_mask(radius):\n",
    "        y, x = np.ogrid[-radius: radius+1, -radius: radius+1]\n",
    "        mask = x**2 + y**2 <= radius**2\n",
    "        return mask\n",
    "\n",
    "    # Pad the image to handle edge cases\n",
    "    padded_image = np.pad(image, radius, mode='constant', constant_values=0)\n",
    "    zoomed_image = zoom(padded_image, zoom_factor, order=3)\n",
    "    zoomed_atoms = [(x * zoom_factor + radius * zoom_factor, y * zoom_factor + radius * zoom_factor) for x, y in atoms]\n",
    "\n",
    "    num_atoms = len(atoms)\n",
    "    circular_mask_region = circular_mask(radius)\n",
    "    intensities = []\n",
    "\n",
    "    for idx, atom in enumerate(zoomed_atoms):\n",
    "        x, y = atom\n",
    "        x, y = int(x), int(y)\n",
    "        intensity_region = zoomed_image[y - radius:y + radius + 1, x - radius:x + radius + 1]\n",
    "\n",
    "        if norm_size is not None:\n",
    "            norm_region = zoomed_image[y - norm_size:y + norm_size + 1, x - norm_size:x + norm_size + 1]\n",
    "            norm_factor = (norm_region.sum() - intensity_region[circular_mask_region].sum()) / np.sum(~circular_mask_region)\n",
    "            normalized_intensity = intensity_region[circular_mask_region] / norm_factor\n",
    "            intensities.append(normalized_intensity)\n",
    "        else:\n",
    "            intensities.append(intensity_region[circular_mask_region])\n",
    "    \n",
    "    return intensities\n",
    "\n",
    "\n",
    "def plot_atom_histograms(results, n_bins=50):\n",
    "    colors = {'W': ['blue', 'cyan'], 'Se': ['green', 'lime'], 'S': ['red', 'magenta']}\n",
    "    \n",
    "    for i, result in results.items():\n",
    "        intensities = {el: [np.concatenate(result['intensities'][el][0]) if result['intensities'][el][0] else np.array([]),\n",
    "                            np.concatenate(result['intensities'][el][1]) if result['intensities'][el][1] else np.array([])]\n",
    "                       for el in colors}\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(20, 8))\n",
    "        # Plot the image with centroids\n",
    "        axs[0].imshow(result['image'], cmap='gray')\n",
    "        for element, colors_list in colors.items():\n",
    "            for idx, (color, layer) in enumerate(zip(colors_list, result['centroids'][element])):\n",
    "                if layer.size > 0:\n",
    "                    axs[0].scatter(layer[:, 1], layer[:, 0], color=color, label=f'{element} Layer {idx + 1} Centroids', alpha=0.6)\n",
    "        axs[0].axis('off')\n",
    "        axs[0].legend(loc='upper right', fontsize=8)\n",
    "\n",
    "        # Plot histograms and KDEs for both layers\n",
    "        for j, layer in enumerate(['Layer 1', 'Layer 2']):\n",
    "            for element, (color1, color2) in colors.items():\n",
    "                color = color1 if j == 0 else color2\n",
    "                layer_intensities = intensities[element][j]\n",
    "                if layer_intensities.size > 0:\n",
    "                    density = gaussian_kde(layer_intensities.flatten())\n",
    "                    x = np.linspace(min(layer_intensities.flatten()), max(layer_intensities.flatten()), 1000)\n",
    "                    axs[j + 1].hist(layer_intensities.flatten(), bins=n_bins, color=color, alpha=0.5, label=f'{element} {layer} Intensities', density=False)\n",
    "                    axs[j + 1].plot(x, density(x), color=color)\n",
    "            axs[j + 1].set_title(f'{layer} Intensities')\n",
    "            axs[j + 1].set_xlabel('Intensity')\n",
    "            axs[j + 1].set_ylabel('Density')\n",
    "            axs[j + 1].legend(loc='upper right', fontsize=8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vegards_law(results):\n",
    "    S_atoms = []\n",
    "    Se_atoms = []\n",
    "    W_atoms = []\n",
    "\n",
    "    for result in results:\n",
    "        for element in results[result][\"centroids\"]:\n",
    "            for layer in results[result][\"centroids\"][element]:\n",
    "                if element == \"S\":\n",
    "                    S_atoms.append(layer)\n",
    "                elif element == \"Se\":\n",
    "                    Se_atoms.append(layer)\n",
    "                else:\n",
    "                    W_atoms.append(layer)\n",
    "\n",
    "\n",
    "    WS2_lattice = 3.15 # Angstrom, https://www.hqgraphene.com/WS2.php\n",
    "    WSe2_lattice = 3.28 # Angstrom, https://www.hqgraphene.com/WSe2.php\n",
    "    WSSe_lattice = 3.24 # Angstrom\n",
    "\n",
    "    # Vegard's law\n",
    "\n",
    "    def vegards_law(a1, a2, x1, x2):\n",
    "        return x1*a1 + x2*a2\n",
    "\n",
    "    x_Se = np.linspace(0,1,11)/2\n",
    "    ideal_lattice = vegards_law(WS2_lattice, WSSe_lattice, 1-x_Se, x_Se)\n",
    "\n",
    "    # plot the ideal lattice\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(x_Se, ideal_lattice, 'k--', label='Vegard\\'s Law')\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for S, Se, W in zip(S_atoms, Se_atoms, W_atoms):\n",
    "        # calculate stoichiometry:\n",
    "        n_S = len(S)\n",
    "        n_Se = len(Se)\n",
    "        \n",
    "        x_Se = n_Se / (n_S * (2 + n_Se/n_S))\n",
    "\n",
    "        # calculate the standard error of the mean\n",
    "        std_err_stoic = np.sqrt((x_Se * (1 - x_Se)) / (n_S + n_Se))\n",
    "\n",
    "        tree = KDTree(W)\n",
    "        distances, indices = tree.query(W, k=3)\n",
    "\n",
    "        # The first column of distances is zero (distance to itself), we want the next two columns\n",
    "        nearest_distances = distances[:, 1:4] * pixel_size # angstroms\n",
    "        # keep distances between 2.5 and 4.0\n",
    "        nearest_distances = nearest_distances[(nearest_distances > 2.5) & (nearest_distances < 4.0)]\n",
    "        avg_distance = np.mean(nearest_distances)\n",
    "        std_distance = np.std(nearest_distances)\n",
    "        std_error = std_distance / np.sqrt(len(nearest_distances))\n",
    "\n",
    "        if x_Se < 0.07:\n",
    "            color = '#1f77b4'\n",
    "            label = 'bottom layer'\n",
    "            plt.errorbar(x_Se, avg_distance, yerr=std_error, xerr=std_err_stoic,capsize=5, c=color, fmt='o', label=label if i == 0 else None)\n",
    "            i = i + 1\n",
    "        else:\n",
    "            color = '#d62728'\n",
    "            label = 'top layer'\n",
    "            plt.errorbar(x_Se, avg_distance, yerr=std_error, xerr=std_err_stoic,capsize=5, c=color, fmt='o', label=label if j == 0 else None)\n",
    "            j = j + 1\n",
    "\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Lattice Constant (Angstrom)')\n",
    "    plt.legend(loc='lower right', fontsize=12)\n",
    "    plt.xlim(0,0.32)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Vegards_law.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gaussian_mixture(metric, n_components):\n",
    "    gmm = GaussianMixture(n_components=n_components)\n",
    "    gmm.fit(metric.reshape(-1, 1))\n",
    "    means = gmm.means_.flatten()\n",
    "    stds = np.sqrt(gmm.covariances_).flatten()\n",
    "    weights = gmm.weights_\n",
    "    log_likelihood = gmm.score(metric.reshape(-1, 1)) * len(metric)\n",
    "    return means, stds, weights, log_likelihood\n",
    "\n",
    "def plot_histogram_with_gaussian(ax, metric, title, n_components, color_bounds):\n",
    "    n, bins, patches = ax.hist(metric, bins=20, density=True, edgecolor='k', alpha=1)\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    norm_dist = mcolors.Normalize(color_bounds[0], color_bounds[1])\n",
    "    cmap = cm.get_cmap(\"viridis\")\n",
    "    for c, p in zip(bin_centers, patches):\n",
    "        plt.setp(p, 'facecolor', cmap(norm_dist(c)))\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    means, stds, weights, log_likelihood = fit_gaussian_mixture(metric, n_components)\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    x = np.linspace(xmin, xmax, 1000)\n",
    "    # total_pdf = sum(weight * (1 / (std * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean) / std) ** 2)\n",
    "    #                 for mean, std, weight in zip(means, stds, weights))\n",
    "    total_pdf = np.zeros_like(x)\n",
    "    for mean, std, weight in zip(means, stds, weights):\n",
    "        print('mean:', mean)\n",
    "        pdf = weight * (1 / (std * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean) / std) ** 2)\n",
    "        total_pdf += pdf\n",
    "        ax.plot(x, pdf, '--', c='k', linewidth=2)\n",
    "\n",
    "    ax.plot(x, total_pdf, 'k-', linewidth=3, alpha=0.8)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_xlim(np.min(metric), np.max(metric))\n",
    "    return means, stds, weights, x, total_pdf, log_likelihood\n",
    "\n",
    "def plot_moire_relationship(results, image_number=0, n_gauss=[1, 1]):\n",
    "    def get_layer_data(layer):\n",
    "        return np.vstack(results[image_number][\"centroids\"][layer][0]), np.vstack(results[image_number][\"centroids\"][layer][1])\n",
    "\n",
    "    Se_0, Se_1 = get_layer_data(\"Se\")\n",
    "    W_0, W_1 = get_layer_data(\"W\")\n",
    "    S_0, S_1 = get_layer_data(\"S\")\n",
    "\n",
    "    if Se_0.shape[0] > Se_1.shape[0]:\n",
    "        Se_bottom, W_bottom, S_bottom, W_top = Se_0, W_0, S_0, W_1\n",
    "    else:\n",
    "        Se_bottom, W_bottom, S_bottom, W_top = Se_1, W_1, S_1, W_0\n",
    "\n",
    "    top_tree, bottom_tree = KDTree(W_top), KDTree(W_bottom)\n",
    "    grid_density = 512\n",
    "    x = np.linspace(0, 512, grid_density)\n",
    "    y = np.linspace(0, 512, grid_density)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    points = np.vstack((X.ravel(), Y.ravel())).T\n",
    "    distances = np.linalg.norm(W_top[top_tree.query(points)[1]] - W_top[top_tree.query(points)[1]], axis=1)\n",
    "    distances = distances.reshape((grid_density, grid_density)) * pixel_size\n",
    "    moire_sites = griddata((X.ravel(), Y.ravel()), distances.ravel(), (Se_bottom[:, 1], Se_bottom[:, 0]), method='cubic')\n",
    "\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(20, 5))\n",
    "    axs[0].imshow(results[image_number][\"image\"], cmap='gray')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    cmap, norm_dist = plt.get_cmap('viridis'), mcolors.Normalize(vmin=distances.min(), vmax=distances.max())\n",
    "    img = axs[1].imshow(distances, cmap=cmap, norm=norm_dist, extent=(0, 512, 512, 0))\n",
    "    axs[1].scatter(Se_bottom[:, 1], Se_bottom[:, 0], s=30, edgecolors='k', c='orange', linewidths=0.5, alpha=1)\n",
    "    axs[1].axis('off')\n",
    "    plt.colorbar(img, ax=axs[1], label='Distance $\\AA$')\n",
    "\n",
    "    color_bounds = [distances.min(), distances.max()]\n",
    "    print('Se-site distribution')\n",
    "    plot_histogram_with_gaussian(axs[2], moire_sites.ravel(), 'Se Site Distribution', n_gauss[0], color_bounds)\n",
    "    print('Moiré distance distribution')\n",
    "    plot_histogram_with_gaussian(axs[3], distances.ravel(), 'Moiré Distance Distribution', n_gauss[1], color_bounds)\n",
    "\n",
    "    x_common = np.linspace(min(axs[2].get_xlim()[0], axs[3].get_xlim()[0]), max(axs[2].get_xlim()[1], axs[3].get_xlim()[1]), 1000)\n",
    "    pdf_moire = sum(weight * (1 / (std * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x_common - mean) / std) ** 2)\n",
    "                    for mean, std, weight in zip(*fit_gaussian_mixture(moire_sites.ravel(), n_gauss[0])[:3]))\n",
    "    pdf_distances = sum(weight * (1 / (std * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x_common - mean) / std) ** 2)\n",
    "                        for mean, std, weight in zip(*fit_gaussian_mixture(distances.ravel(), n_gauss[1])[:3]))\n",
    "    axs[4].hlines(0, x_common.min(), x_common.max(), color='k', linestyle='--')\n",
    "    axs[4].plot(x_common, pdf_moire, 'b-', linewidth=2, label='Se Site Distribution')\n",
    "    axs[4].plot(x_common, pdf_distances, 'r-', linewidth=2, label='Moiré Distance Distribution')\n",
    "    axs[4].plot(x_common, pdf_moire - pdf_distances, 'g-', linewidth=2, label='Difference')\n",
    "    axs[4].legend(loc='upper right')\n",
    "    axs[4].set_title('Difference', fontsize=14)\n",
    "    axs[4].set_xlim(0, 6.0)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(ax, metric, color_bounds):\n",
    "    n, bins, patches = ax.hist(metric, bins=20, density=True, edgecolor='k', alpha=1)\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    norm_dist = mcolors.Normalize(color_bounds[0], color_bounds[1])\n",
    "    cmap = cm.get_cmap(\"viridis\")\n",
    "    for c, p in zip(bin_centers, patches):\n",
    "        plt.setp(p, 'facecolor', cmap(norm_dist(c)))\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    ax.set_xlim(np.min(metric), np.max(metric))\n",
    "    return\n",
    "\n",
    "def plot_moire_distances(results, image_number=0):\n",
    "    def get_layer_data(layer):\n",
    "        return np.vstack(results[image_number][\"centroids\"][layer][0]), np.vstack(results[image_number][\"centroids\"][layer][1])\n",
    "\n",
    "    Se_0, Se_1 = get_layer_data(\"Se\")\n",
    "    W_0, W_1 = get_layer_data(\"W\")\n",
    "    S_0, S_1 = get_layer_data(\"S\")\n",
    "\n",
    "    if Se_0.shape[0] > Se_1.shape[0]:\n",
    "        Se_bottom, W_bottom, S_bottom, W_top = Se_0, W_0, S_0, W_1\n",
    "        Se_top, S_top = Se_1, S_1\n",
    "    else:\n",
    "        Se_bottom, W_bottom, S_bottom, W_top = Se_1, W_1, S_1, W_0\n",
    "        Se_top, S_top = Se_0, S_0\n",
    "    chalc_top = np.vstack((S_top, Se_top))\n",
    "    chalc_bottom = np.vstack((S_bottom, Se_bottom))\n",
    "    W_top_tree, W_bottom_tree = KDTree(W_top), KDTree(W_bottom)\n",
    "    chalc_top_tree, chalc_bottom_tree = KDTree(chalc_top), KDTree(chalc_bottom)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(2, 5, figsize=(20, 5))\n",
    "    ax[0,0].imshow(results[image_number][\"image\"].T, cmap='gray')\n",
    "    ax[0,0].set_title('Image')\n",
    "\n",
    "    grid_density = 512\n",
    "    x = np.linspace(0, 512, grid_density)\n",
    "    y = np.linspace(0, 512, grid_density)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    points = np.vstack((X.ravel(), Y.ravel())).T\n",
    "\n",
    "\n",
    "    distance_arrays = []\n",
    "\n",
    "    # W_top - W_bottom distances\n",
    "    distances = np.linalg.norm(W_top[W_top_tree.query(points)[1]] - W_bottom[W_bottom_tree.query(points)[1]], axis=1)\n",
    "    distances = distances.reshape((grid_density, grid_density)) * pixel_size\n",
    "    moire_sites = griddata((X.ravel(), Y.ravel()), distances.ravel(), (Se_bottom[:, 1], Se_bottom[:, 0]), method='cubic')\n",
    "    ax[0,1].imshow(distances, cmap='viridis', extent=(0, 512, 512, 0))\n",
    "    ax[0,1].set_title('W-W Distances')\n",
    "    color_bounds = [distances.min(), distances.max()]\n",
    "    plot_histogram(ax[1,1], moire_sites.ravel(), color_bounds)\n",
    "    distance_arrays.append(distances)\n",
    "\n",
    "    # W_top - Chalc_bottom distances\n",
    "    distances = np.linalg.norm(W_top[W_top_tree.query(points)[1]] - chalc_bottom[chalc_bottom_tree.query(points)[1]], axis=1)\n",
    "    distances = distances.reshape((grid_density, grid_density)) * pixel_size\n",
    "    moire_sites = griddata((X.ravel(), Y.ravel()), distances.ravel(), (Se_bottom[:, 1], Se_bottom[:, 0]), method='cubic')\n",
    "    ax[0,2].imshow(distances, cmap='viridis', extent=(0, 512, 512, 0))\n",
    "    ax[0,2].set_title('W-Chalc Distances')\n",
    "    color_bounds = [distances.min(), distances.max()]\n",
    "    plot_histogram(ax[1,2], moire_sites.ravel(), color_bounds)\n",
    "    distance_arrays.append(distances)\n",
    "\n",
    "    # Chalc_top - W_bottom distances\n",
    "    distances = np.linalg.norm(chalc_top[chalc_top_tree.query(points)[1]] - W_bottom[W_bottom_tree.query(points)[1]], axis=1)\n",
    "    distances = distances.reshape((grid_density, grid_density)) * pixel_size\n",
    "    moire_sites = griddata((X.ravel(), Y.ravel()), distances.ravel(), (Se_bottom[:, 1], Se_bottom[:, 0]), method='cubic')\n",
    "    ax[0,3].imshow(distances, cmap='viridis', extent=(0, 512, 512, 0))\n",
    "    ax[0,3].set_title('Chalc-W Distances')\n",
    "    color_bounds = [distances.min(), distances.max()]\n",
    "    plot_histogram(ax[1,3], moire_sites.ravel(), color_bounds)\n",
    "    distance_arrays.append(distances)\n",
    "\n",
    "    # Chalc_top - Chalc_bottom distances\n",
    "    distances = np.linalg.norm(chalc_top[chalc_top_tree.query(points)[1]] - chalc_bottom[chalc_bottom_tree.query(points)[1]], axis=1)\n",
    "    distances = distances.reshape((grid_density, grid_density)) * pixel_size\n",
    "    moire_sites = griddata((X.ravel(), Y.ravel()), distances.ravel(), (Se_bottom[:, 1], Se_bottom[:, 0]), method='cubic')\n",
    "    ax[0,4].imshow(distances, cmap='viridis', extent=(0, 512, 512, 0))\n",
    "    ax[0,4].set_title('Chalc-Chalc Distances')\n",
    "    color_bounds = [distances.min(), distances.max()]\n",
    "    plot_histogram(ax[1,4], moire_sites.ravel(), color_bounds)\n",
    "    distance_arrays.append(distances)\n",
    "\n",
    "    for a in ax.ravel()[:5]:\n",
    "        a.axis('off')\n",
    "    ax[1,0].axis('off')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return distance_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(image_number, results, n_classes):\n",
    "    # Plot the original image\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(results[image_number]['image'], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Original Image')\n",
    "\n",
    "    # Create a figure with subplots for masks and probabilities\n",
    "    fig, axs = plt.subplots(2, n_classes, figsize=(20, 10))\n",
    "\n",
    "    # Plot the masks\n",
    "    for j in range(n_classes):\n",
    "        axs[0, j].imshow(results[image_number]['mask'][j], cmap='gray')\n",
    "        axs[0, j].axis('off')\n",
    "        axs[0, j].set_title(f'Mask {j+1}')\n",
    "\n",
    "    # Plot the probabilities\n",
    "    for j in range(n_classes):\n",
    "        axs[1, j].imshow(results[image_number]['probability'][j], cmap='plasma')\n",
    "        axs[1, j].axis('off')\n",
    "        axs[1, j].set_title(f'Probability {j+1}')\n",
    "\n",
    "    plt.suptitle(f'Results for Image {image_number}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image 1 ~ 20 deg twist (or -40, however you want to think about it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data = np.load('./Exp_data/WSSe_haadf.npz')\n",
    "im_array = exp_data['im_array']\n",
    "pixel_size = exp_data['pixel_size']\n",
    "\n",
    "im_array = np.sum(im_array, axis = 0)\n",
    "\n",
    "# im_array = gaussian_filter(im_array, sigma=1)\n",
    "im_array = np.power(im_array,2.6)\n",
    "im_array = im_array - np.min(im_array)\n",
    "\n",
    "im_array = im_array / np.max(im_array)\n",
    "\n",
    "\n",
    "print(f\"Pixel size: {pixel_size.astype(float)} m/pix\")\n",
    "plt.figure()\n",
    "plt.imshow(im_array, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "exp_hist, bins = np.histogram(im_array.ravel(), bins=256, range=(0.0, 1.0))\n",
    "\n",
    "selected_images = []\n",
    "for i, data in enumerate(train_loader):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    images = data[0].numpy()  # Convert batch of images to numpy array\n",
    "    selected_images.append(images)\n",
    "\n",
    "selected_images = np.concatenate(selected_images, axis=0)\n",
    "selected_images = selected_images / selected_images.max()\n",
    "selected_images_raveled = selected_images.ravel()\n",
    "train_hist, _ = np.histogram(selected_images_raveled, bins=256, range=(0.0, 1.0))\n",
    "\n",
    "# Plot the histograms\n",
    "plt.figure()\n",
    "plt.hist(im_array.ravel(), bins=256, range=(0.0, 1.0), fc='k', ec='k', alpha=0.5, label='Experimental Image')\n",
    "plt.plot(bins[:-1], train_hist, 'r', alpha=0.5, label='Training Images')\n",
    "plt.legend()\n",
    "plt.xlabel('Intensity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Experimental Image vs Training Images')\n",
    "plt.show()\n",
    "\n",
    "n_crops = 10\n",
    "images = dg.shotgun_crop(im_array, crop_size=512, n_crops = n_crops, roi = None)\n",
    "# normalize each image in images to 0,1\n",
    "for i in range(n_crops):\n",
    "    images[i] = images[i] - np.min(images[i])\n",
    "    images[i] = images[i] / np.max(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import center_of_mass\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of classes and crops\n",
    "n_classes = 6\n",
    "n_crops = len(images)\n",
    "results = {}\n",
    "\n",
    "# Iterate over the images\n",
    "for i, im in enumerate(images):\n",
    "    # Model Prediction\n",
    "    im_tensor = torch.tensor(im.astype(np.float32)).unsqueeze(0).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        probability = model(im_tensor)\n",
    "    probability = probability.squeeze().cpu().numpy()\n",
    "    probability = probability - np.min(probability)\n",
    "    probability = probability / np.max(probability)\n",
    "    masks = (probability > 0.5).astype(float)\n",
    "\n",
    "    centroids = {'W': [], 'S': [], 'Se': []}\n",
    "    intensities = {'W': [], 'S': [], 'Se': []}\n",
    "    summed_prob = {'W': [], 'S': [], 'Se': []}\n",
    "\n",
    "    for j, layer in enumerate(masks):\n",
    "        if j == 0 or j == 3:\n",
    "            element = 'S'\n",
    "        elif j == 1 or j == 4:\n",
    "            element = 'Se'\n",
    "        else:\n",
    "            element = 'W'\n",
    "\n",
    "        labeled_array, num_features = ndimage.label(layer)\n",
    "        layer_centroids = np.array(center_of_mass(layer, labeled_array, range(1, num_features + 1)))\n",
    "        centroids[element].append(layer_centroids)\n",
    "\n",
    "        layer_intensities = extract_intensity(im, layer_centroids, radius=7, zoom_factor=2, norm_size=None)\n",
    "        intensities[element].append(layer_intensities)\n",
    "\n",
    "        # Summing probabilities\n",
    "        layer_summed_prob = [np.sum(probability[j][tuple(np.round(c).astype(int))]) for c in layer_centroids]\n",
    "        summed_prob[element].append(layer_summed_prob)\n",
    "\n",
    "    # Process each group of layers separately\n",
    "    for k in range(2):\n",
    "        # Select the appropriate layers\n",
    "        layers = [(k*3), (k*3)+1, (k*3)+2]\n",
    "        # Flatten lists for easier manipulation\n",
    "        centroids_S = np.array(centroids['S'][k])\n",
    "        centroids_Se = np.array(centroids['Se'][k])\n",
    "        summed_prob_S = np.array(summed_prob['S'][k])\n",
    "        summed_prob_Se = np.array(summed_prob['Se'][k])\n",
    "\n",
    "        # Count W and S+Se atoms within this group\n",
    "        n_W = len(centroids['W'][k])\n",
    "        n_S_Se = len(centroids_S) + len(centroids_Se)\n",
    "        excess_atoms = n_S_Se - n_W\n",
    "\n",
    "        if excess_atoms > 0:\n",
    "            # Check for proximity and delete close ones first\n",
    "            distances = cdist(centroids_S, centroids_Se)\n",
    "            close_pairs = np.argwhere(distances < 0.001)  # Adjust the threshold distance as needed\n",
    "\n",
    "            S_mask = np.ones(len(centroids_S), dtype=bool)\n",
    "            Se_mask = np.ones(len(centroids_Se), dtype=bool)\n",
    "\n",
    "            for (idx_S, idx_Se) in close_pairs:\n",
    "                if excess_atoms <= 0:\n",
    "                    break\n",
    "                if not S_mask[idx_S] or not Se_mask[idx_Se]:\n",
    "                    continue\n",
    "                if summed_prob_S[idx_S] < summed_prob_Se[idx_Se]:\n",
    "                    S_mask[idx_S] = False\n",
    "                else:\n",
    "                    Se_mask[idx_Se] = False\n",
    "                excess_atoms -= 1\n",
    "\n",
    "            # If there are still atoms to delete, sort by summed_prob and delete the lowest ones\n",
    "            if excess_atoms > 0:\n",
    "                all_probs = np.concatenate((summed_prob_S[S_mask], summed_prob_Se[Se_mask]))\n",
    "                all_centroids = np.concatenate((centroids_S[S_mask], centroids_Se[Se_mask]), axis=0)\n",
    "                sorted_indices = np.argsort(all_probs)\n",
    "\n",
    "                deletion_mask = np.ones(len(all_centroids), dtype=bool)\n",
    "                for idx in sorted_indices:\n",
    "                    if excess_atoms <= 0:\n",
    "                        break\n",
    "                    deletion_mask[idx] = False\n",
    "                    excess_atoms -= 1\n",
    "\n",
    "                # Update centroids after deletion\n",
    "                valid_indices_S = deletion_mask[:len(centroids_S[S_mask])]\n",
    "                valid_indices_Se = deletion_mask[len(centroids_S[S_mask]):]\n",
    "\n",
    "                centroids_S = centroids_S[S_mask][valid_indices_S]\n",
    "                centroids_Se = centroids_Se[Se_mask][valid_indices_Se]\n",
    "\n",
    "            # Update intensities after deletion\n",
    "            intensities['S'][k] = extract_intensity(im, centroids_S, radius=7, zoom_factor=2, norm_size=None)\n",
    "            intensities['Se'][k] = extract_intensity(im, centroids_Se, radius=7, zoom_factor=2, norm_size=None)\n",
    "            centroids['S'][k] = centroids_S\n",
    "            centroids['Se'][k] = centroids_Se\n",
    "\n",
    "    results[i] = {\n",
    "        \"image\": im,\n",
    "        \"mask\": masks,\n",
    "        \"probability\": probability,\n",
    "        \"centroids\": centroids,\n",
    "        \"intensities\": intensities\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = 0  # Replace with the desired image number\n",
    "visualize_predictions(image_number, results, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vegards_law(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_atom_histograms(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(ax, metric, color_bounds):\n",
    "    n, bins, patches = ax.hist(metric, bins=20, density=True, edgecolor='k', alpha=1)\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    norm_dist = mcolors.Normalize(color_bounds[0], color_bounds[1])\n",
    "    cmap = cm.get_cmap(\"inferno\")\n",
    "    for c, p in zip(bin_centers, patches):\n",
    "        plt.setp(p, 'facecolor', cmap(norm_dist(c)))\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    ax.set_xlim(np.min(metric), np.max(metric))\n",
    "    return\n",
    "\n",
    "def plot_moire_distances(results, image_number=0):\n",
    "    def get_layer_data(layer):\n",
    "        return np.vstack(results[image_number][\"centroids\"][layer][0]), np.vstack(results[image_number][\"centroids\"][layer][1])\n",
    "\n",
    "    Se_0, Se_1 = get_layer_data(\"Se\")\n",
    "    W_0, W_1 = get_layer_data(\"W\")\n",
    "    S_0, S_1 = get_layer_data(\"S\")\n",
    "\n",
    "    if Se_0.shape[0] > Se_1.shape[0]:\n",
    "        Se_bottom, W_bottom, S_bottom, W_top = Se_0, W_0, S_0, W_1\n",
    "        Se_top, S_top = Se_1, S_1\n",
    "    else:\n",
    "        Se_bottom, W_bottom, S_bottom, W_top = Se_1, W_1, S_1, W_0\n",
    "        Se_top, S_top = Se_0, S_0\n",
    "    chalc_top = np.vstack((S_top, Se_top))\n",
    "    chalc_bottom = np.vstack((S_bottom, Se_bottom))\n",
    "    W_top_tree, W_bottom_tree = KDTree(W_top), KDTree(W_bottom)\n",
    "    chalc_top_tree, chalc_bottom_tree = KDTree(chalc_top), KDTree(chalc_bottom)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(1, 5, figsize=(20, 5), dpi=300)\n",
    "    ax[0].imshow(results[image_number][\"image\"], cmap='gray')\n",
    "    ax[0].set_title('Image')\n",
    "\n",
    "    grid_density = 512\n",
    "    x = np.linspace(0, 512, grid_density)\n",
    "    y = np.linspace(0, 512, grid_density)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    points = np.vstack((X.ravel(), Y.ravel())).T\n",
    "\n",
    "\n",
    "    distance_arrays = []\n",
    "\n",
    "    # W_top - W_bottom distances\n",
    "    distances = np.linalg.norm(W_top[W_top_tree.query(points)[1]].reshape(-1,2) - W_bottom[W_bottom_tree.query(points)[1]].reshape(-1,2), axis=1)\n",
    "    distances = distances.reshape((grid_density, grid_density)) * pixel_size\n",
    "    moire_sites = griddata((X.ravel(), Y.ravel()), distances.ravel(), (Se_bottom[:, 1], Se_bottom[:, 0]), method='cubic')\n",
    "    ax[1].imshow(distances, cmap='inferno', extent=(0, 512, 512, 0), vmin=0, vmax=6)\n",
    "    ax[1].set_title('W-W Distances')\n",
    "    color_bounds = [distances.min(), distances.max()]\n",
    "    distance_arrays.append(distances)\n",
    "\n",
    "    # W_top - Chalc_bottom distances\n",
    "    distances = np.linalg.norm(W_top[W_top_tree.query(points)[1]].reshape(-1,2) - chalc_bottom[chalc_bottom_tree.query(points)[1]].reshape(-1,2), axis=1)\n",
    "    distances = distances.reshape((grid_density, grid_density)) * pixel_size\n",
    "    moire_sites = griddata((X.ravel(), Y.ravel()), distances.ravel(), (Se_bottom[:, 1], Se_bottom[:, 0]), method='cubic')\n",
    "    ax[2].imshow(distances, cmap='inferno', extent=(0, 512, 512, 0), vmin=0, vmax=6)\n",
    "    ax[2].set_title('W-Chalc Distances')\n",
    "    color_bounds = [distances.min(), distances.max()]\n",
    "    distance_arrays.append(distances)\n",
    "\n",
    "    # Chalc_top - W_bottom distances\n",
    "    distances = np.linalg.norm(chalc_top[chalc_top_tree.query(points)[1]].reshape(-1,2) - W_bottom[W_bottom_tree.query(points)[1]].reshape(-1,2), axis=1)\n",
    "    distances = distances.reshape((grid_density, grid_density)) * pixel_size\n",
    "    moire_sites = griddata((X.ravel(), Y.ravel()), distances.ravel(), (Se_bottom[:, 1], Se_bottom[:, 0]), method='cubic')\n",
    "    ax[3].imshow(distances, cmap='inferno', extent=(0, 512, 512, 0), vmin=0, vmax=6)\n",
    "    ax[3].set_title('Chalc-W Distances')\n",
    "    color_bounds = [distances.min(), distances.max()]\n",
    "    distance_arrays.append(distances)\n",
    "\n",
    "    # Chalc_top - Chalc_bottom distances\n",
    "    distances = np.linalg.norm(chalc_top[chalc_top_tree.query(points)[1]].reshape(-1,2) - chalc_bottom[chalc_bottom_tree.query(points)[1]].reshape(-1,2), axis=1)\n",
    "    distances = distances.reshape((grid_density, grid_density)) * pixel_size\n",
    "    moire_sites = griddata((X.ravel(), Y.ravel()), distances.ravel(), (Se_bottom[:, 1], Se_bottom[:, 0]), method='cubic')\n",
    "    ax[4].imshow(distances, cmap='inferno', extent=(0, 512, 512, 0), vmin=0, vmax=6)\n",
    "    ax[4].set_title('Chalc-Chalc Distances')\n",
    "    color_bounds = [distances.min(), distances.max()]\n",
    "    distance_arrays.append(distances)\n",
    "\n",
    "    for a in ax.ravel():\n",
    "        a.axis('off')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return distance_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_list = plot_moire_distances(results, image_number=0)\n",
    "\n",
    "distance_arrays = np.array(distance_list)\n",
    "# normalize, keep norm factor\n",
    "norm_shift = distance_arrays.min()\n",
    "distance_arrays = distance_arrays - distance_arrays.min()\n",
    "norm_factor = distance_arrays.max()\n",
    "distance_arrays = distance_arrays / distance_arrays.max()\n",
    "# slices to include in the array:\n",
    "distance_arrays = distance_arrays[[0,1,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(results[0][\"image\"], cmap='gray', extent=(0, 512, 512, 0)*pixel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below,the lowest values (near zero) correspond to 2H stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from sklearn.neighbors import KDTree\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import viridis\n",
    "from matplotlib.cm import plasma\n",
    "from matplotlib.cm import jet\n",
    "from matplotlib.cm import inferno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number=0\n",
    "def get_layer_data(layer):\n",
    "    return np.vstack(results[image_number][\"centroids\"][layer][0]), np.vstack(results[image_number][\"centroids\"][layer][1])\n",
    "\n",
    "Se_0, Se_1 = get_layer_data(\"Se\")\n",
    "W_0, W_1 = get_layer_data(\"W\")\n",
    "S_0, S_1 = get_layer_data(\"S\")\n",
    "\n",
    "if Se_0.shape[0] > Se_1.shape[0]:\n",
    "    Se_bottom, W_bottom, S_bottom, W_top = Se_0, W_0, S_0, W_1\n",
    "else:\n",
    "    Se_bottom, W_bottom, S_bottom, W_top = Se_1, W_1, S_1, W_0\n",
    "\n",
    "map_2H = np.sqrt(distance_arrays[1]**2 + distance_arrays[2]**2)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# First subplot\n",
    "im1 = ax1.imshow(map_2H, cmap='viridis')\n",
    "ax1.scatter(Se_bottom[:, 1], Se_bottom[:, 0], s=20, c='orange', linewidths=0.5, edgecolors='k', label='Se Sites')\n",
    "fig.colorbar(im1, ax=ax1)\n",
    "ax1.axis('off')\n",
    "\n",
    "grid_density = 512\n",
    "x = np.linspace(0, 512, grid_density)\n",
    "y = np.linspace(0, 512, grid_density)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "points = np.vstack((X.ravel(), Y.ravel())).T\n",
    "Se_tree = KDTree(Se_bottom)\n",
    "moire_sites = griddata((X.ravel(), Y.ravel()), map_2H.ravel(), (Se_bottom[:, 1], Se_bottom[:, 0]), method='cubic')\n",
    "\n",
    "\n",
    "binedges = np.histogram_bin_edges(np.concatenate([moire_sites.ravel(), map_2H.ravel()]), bins=50)\n",
    "hist_moire_sites, _ = np.histogram(moire_sites.ravel(), bins=binedges, density=False)\n",
    "hist_map_2H, _ = np.histogram(map_2H.ravel(), bins=binedges, density=False)\n",
    "\n",
    "bin_widths = np.diff(binedges)\n",
    "hist_moire_sites_density = hist_moire_sites / (np.sum(hist_moire_sites) * bin_widths)\n",
    "hist_map_2H_density = hist_map_2H / (np.sum(hist_map_2H) * bin_widths)\n",
    "\n",
    "# Create colormap\n",
    "norm = Normalize(vmin=np.min(map_2H), vmax=np.max(map_2H))\n",
    "colors = viridis(norm(binedges[:-1]))\n",
    "ax2.bar(binedges[:-1], hist_map_2H_density, width=bin_widths, color=colors, edgecolor='k', alpha=1, label='map_ravel')\n",
    "ax2.step(binedges[:-1], hist_moire_sites_density, where='mid', color='#E69F00', linewidth=3, label='Se_site_distribution')\n",
    "\n",
    "ax2.legend()\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do this in a batch:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get centroid data for a given layer\n",
    "def get_layer_data(results, image_number, layer):\n",
    "    return (np.vstack(results[image_number][\"centroids\"][layer][0]), \n",
    "            np.vstack(results[image_number][\"centroids\"][layer][1]))\n",
    "\n",
    "# Lists to store raw histogram counts for each image\n",
    "master_atom_histogram = []\n",
    "master_2H_map_histogram = []\n",
    "master_distance_maps = []\n",
    "\n",
    "# List of image numbers to process\n",
    "image_numbers = [0, 2, 3, 5, 7]\n",
    "\n",
    "for image_number in image_numbers:\n",
    "    # get the distance maps\n",
    "    distance_list = plot_moire_distances(results, image_number=image_number)\n",
    "    distance_maps = np.array(distance_list)\n",
    "\n",
    "    # Calculate 2H map-\n",
    "    map_2H = np.sqrt(distance_maps[1]**2 + distance_maps[2]**2) \n",
    "\n",
    "    # Distance Map Histograms\n",
    "    grid_density = 512\n",
    "    x = np.linspace(0, 512, grid_density)\n",
    "    y = np.linspace(0, 512, grid_density)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    moire_sites = griddata((X.ravel(), Y.ravel()), map_2H.ravel(), (Se_bottom[:, 1], Se_bottom[:, 0]), method='cubic')\n",
    "    distance_map_histogram = map_2H.ravel()\n",
    "\n",
    "    # atom sites\n",
    "    # Determine bottom and top layers\n",
    "    Se_0, Se_1 = get_layer_data(results, image_number, \"Se\")\n",
    "    W_0, W_1 = get_layer_data(results, image_number, \"W\")\n",
    "    S_0, S_1 = get_layer_data(results, image_number, \"S\")\n",
    "    if Se_0.shape[0] > Se_1.shape[0]:\n",
    "        Se_bottom, W_bottom, S_bottom, W_top = Se_0, W_0, S_0, W_1\n",
    "    else:\n",
    "        Se_bottom, W_bottom, S_bottom, W_top = Se_1, W_1, S_1, W_0\n",
    "\n",
    "    atom_site_histogram = moire_sites.ravel()\n",
    "\n",
    "    # save the results to the master\n",
    "    master_atom_histogram.append(atom_site_histogram)\n",
    "    master_2H_map_histogram.append(distance_map_histogram)\n",
    "    master_distance_maps.append(distance_maps)\n",
    "\n",
    "#     fig, ax = plt.subplots(1, 4, sharex=True, sharey=True, figsize=(20, 5), dpi=300)\n",
    "# \n",
    "#     ax[0].imshow(results[image_number][\"image\"], cmap='gray')\n",
    "#     ax[0].set_title('Image')\n",
    "# \n",
    "#     ax[1].imshow(map_2H, cmap='viridis', vmin=0, vmax=4)\n",
    "#     ax[1].scatter(Se_bottom[:, 1], Se_bottom[:, 0], s=20, c='orange', linewidths=0.5, edgecolors='k', label='Se Sites')\n",
    "#     ax[1].set_title('2H Map')\n",
    "# \n",
    "#     # W-C map\n",
    "#     ax[2].imshow(distance_maps[1], cmap='viridis', vmin=0, vmax=4)\n",
    "#     ax[2].set_title('W-C Distances')\n",
    "# \n",
    "#     # C-W map\n",
    "#     ax[3].imshow(distance_maps[2], cmap='viridis', vmin=0, vmax=4)\n",
    "#     ax[3].set_title('C-W Distances')\n",
    "# \n",
    "#     for a in ax:\n",
    "#         a.axis('off')\n",
    "#     fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of image numbers to process\n",
    "cmap = 'inferno'\n",
    "image_numbers = [0, 2, 3, 5, 7]\n",
    "\n",
    "for image_number in [2]:\n",
    "    # get the distance maps\n",
    "    distance_list = plot_moire_distances(results, image_number=image_number)\n",
    "    distance_maps = np.array(distance_list)\n",
    "\n",
    "    # Calculate 2H map-\n",
    "    map_2H = np.sqrt(distance_maps[1]**2 + distance_maps[2]**2) \n",
    "\n",
    "    # Distance Map Histograms\n",
    "    grid_density = 512\n",
    "    x = np.linspace(0, 512, grid_density)\n",
    "    y = np.linspace(0, 512, grid_density)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    moire_sites = griddata((X.ravel(), Y.ravel()), map_2H.ravel(), (Se_bottom[:, 1], Se_bottom[:, 0]), method='cubic')\n",
    "    distance_map_histogram = map_2H.ravel()\n",
    "\n",
    "    # atom sites\n",
    "    # Determine bottom and top layers\n",
    "    Se_0, Se_1 = get_layer_data(results, image_number, \"Se\")\n",
    "    W_0, W_1 = get_layer_data(results, image_number, \"W\")\n",
    "    S_0, S_1 = get_layer_data(results, image_number, \"S\")\n",
    "    if Se_0.shape[0] > Se_1.shape[0]:\n",
    "        Se_bottom, W_bottom, S_bottom, W_top = Se_0, W_0, S_0, W_1\n",
    "    else:\n",
    "        Se_bottom, W_bottom, S_bottom, W_top = Se_1, W_1, S_1, W_0\n",
    "\n",
    "    atom_site_histogram = moire_sites.ravel()\n",
    "\n",
    "    # save the results to the master\n",
    "    master_atom_histogram.append(atom_site_histogram)\n",
    "    master_2H_map_histogram.append(distance_map_histogram)\n",
    "    master_distance_maps.append(distance_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmcrameri import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = 'inferno'\n",
    "# cmap = cm.batlow\n",
    "scatter_color = plt.get_cmap('tab10')(2)\n",
    "# scatter_color = 'red'\n",
    "for i, image_number in enumerate(image_numbers):    \n",
    "    fig, ax = plt.subplots(1, 4, sharex=True, sharey=True, figsize=(20, 5), dpi=300)\n",
    "\n",
    "    # atom sites\n",
    "    # Determine bottom and top layers\n",
    "    Se_0, Se_1 = get_layer_data(results, image_number, \"Se\")\n",
    "    W_0, W_1 = get_layer_data(results, image_number, \"W\")\n",
    "    S_0, S_1 = get_layer_data(results, image_number, \"S\")\n",
    "    if Se_0.shape[0] > Se_1.shape[0]:\n",
    "        Se_bottom, W_bottom, S_bottom, W_top = Se_0, W_0, S_0, W_1\n",
    "    else:\n",
    "        Se_bottom, W_bottom, S_bottom, W_top = Se_1, W_1, S_1, W_0\n",
    "\n",
    "    ax[0].imshow(results[image_number][\"image\"], cmap='gray')\n",
    "    ax[0].set_title('Image')\n",
    "\n",
    "    ax[1].imshow(master_2H_map_histogram[i].reshape((512,512)).T, cmap=cmap, vmin=0.2, vmax=6)\n",
    "    ax[1].scatter(Se_bottom[:, 1], Se_bottom[:, 0], s=100, c=scatter_color, linewidths=2, edgecolors='k', label='Se Sites')\n",
    "    ax[1].set_title('2H Map')\n",
    "\n",
    "    # W-C map\n",
    "    ax[2].imshow(master_distance_maps[i][1].T, cmap=cmap, vmin=0, vmax=6)\n",
    "    ax[2].set_title('W-Chalchogen Distances')\n",
    "\n",
    "    # C-W map\n",
    "    ax[3].imshow(master_distance_maps[i][2].T, cmap=cmap, vmin=0, vmax=6)\n",
    "    ax[3].set_title('Chalcogen-W Distances')\n",
    "\n",
    "    for a in ax:\n",
    "        a.axis('off')\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to normalize the x-axis:\n",
    "lattice_constant = 1\n",
    "n_bins = 100\n",
    "\n",
    "# Concatenate all histograms\n",
    "combined_atom_histogram = np.concatenate(master_atom_histogram)\n",
    "combined_2H_map_histogram = np.concatenate(master_2H_map_histogram)\n",
    "\n",
    "# Compute new bin edges\n",
    "binedges_total = np.histogram_bin_edges(np.concatenate([combined_atom_histogram, combined_2H_map_histogram]), bins=n_bins)\n",
    "hist_atom_total, _ = np.histogram(combined_atom_histogram, bins=binedges_total, density=True)\n",
    "hist_2H_map_total, _ = np.histogram(combined_2H_map_histogram, bins=binedges_total, density=True)  \n",
    "binedges_total = binedges_total / lattice_constant\n",
    "bin_widths_total = np.diff(binedges_total)\n",
    "hist_atom_density_total = hist_atom_total / (np.sum(hist_atom_total) * bin_widths_total)\n",
    "hist_2H_map_density_total = hist_2H_map_total / (np.sum(hist_2H_map_total) * bin_widths_total)\n",
    "\n",
    "normalized_histogram = hist_atom_density_total / hist_2H_map_density_total\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))#, dpi=300)\n",
    "\n",
    "norm = Normalize(vmin=np.min(binedges_total), vmax=np.max(binedges_total))\n",
    "colors_total = inferno(norm(binedges_total[:-1]))\n",
    "ax.bar(binedges_total[:-1], hist_2H_map_density_total, width=bin_widths_total, color=colors_total, edgecolor='k', alpha=1, label='2H Map Density')\n",
    "ax.step(binedges_total, np.append(hist_atom_density_total, hist_atom_density_total[-1]), where='post', color=scatter_color, linewidth=3, label='Se Site Density')\n",
    "ax.plot([binedges_total[0], binedges_total[0]], [0, hist_atom_density_total[0]], color='#E69F00', linewidth=3)\n",
    "for dist in [0,1,np.sqrt(2)]:\n",
    "    ax.axvline(x=dist*3.16, ymin=0, ymax=100, color='k', linestyle = '--', linewidth=3)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlim(-0.1, 6)\n",
    "\n",
    "\n",
    "plt.xlabel('Distance (Angstroms)')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combined_2H_map_histogram\n",
    "\n",
    "# Calculate the standard deviation and IQR\n",
    "std_dev = np.std(data)\n",
    "iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "n = len(data)\n",
    "\n",
    "# Calculate the bandwidth using Silverman's rule of thumb\n",
    "bandwidth = 0.9 * min(std_dev, iqr / 1.34) * n**(-1/5)\n",
    "\n",
    "print(f\"Calculated bandwidth: {bandwidth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import jet\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Kernel density estimation function\n",
    "def compute_kde(data, bandwidth=0.1, n_bins=100, x_min=0, x_max=10):\n",
    "    kde = KernelDensity(bandwidth=bandwidth)\n",
    "    kde.fit(data[:, np.newaxis])\n",
    "    x_d = np.linspace(x_min, x_max, n_bins)\n",
    "    log_dens = kde.score_samples(x_d[:, np.newaxis])\n",
    "    return np.exp(log_dens), x_d\n",
    "\n",
    "# Normalize the x-axis\n",
    "lattice_constant = 1\n",
    "n_bins = 100\n",
    "bandwidth = 0.1\n",
    "\n",
    "# Concatenate all histograms\n",
    "combined_atom_histogram = np.concatenate(master_atom_histogram)\n",
    "combined_2H_map_histogram = np.concatenate(master_2H_map_histogram)\n",
    "\n",
    "# Apply KDE to both histograms\n",
    "kde_atom_density_total, x_d = compute_kde(combined_atom_histogram, bandwidth, n_bins)\n",
    "kde_2H_map_density_total, _ = compute_kde(combined_2H_map_histogram, bandwidth, n_bins)\n",
    "\n",
    "# Compute bin edges for plotting\n",
    "binedges_total = x_d / lattice_constant\n",
    "bin_widths_total = np.diff(binedges_total)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "norm = Normalize(vmin=np.min(binedges_total), vmax=np.max(binedges_total))\n",
    "colors_total = inferno(norm(binedges_total[:-1]))\n",
    "\n",
    "# KDE plot for 2H Map Density\n",
    "ax.plot(binedges_total, kde_2H_map_density_total, color='purple', label='2H Map Density')\n",
    "ax.fill_between(binedges_total, kde_2H_map_density_total, color='purple', alpha=0.3)\n",
    "ax.plot(binedges_total, kde_atom_density_total, color='green', label='Se Site Density')\n",
    "ax.fill_between(binedges_total, kde_atom_density_total, color='green', alpha=0.3)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlim(-0.1, 6)\n",
    "ax.set_ylim(0,0.8)\n",
    "for dist in [0,1,np.sqrt(2)]:\n",
    "    ax.axvline(x=dist*3.16, ymin=0, ymax=100, color='k', linestyle = '--', linewidth=3)\n",
    "\n",
    "plt.xlabel('Distance (Angstroms)')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import inferno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel density estimation function\n",
    "def compute_kde(data, bandwidth=0.1, n_bins=100, x_min=0, x_max=10):\n",
    "    kde = KernelDensity(bandwidth=bandwidth)\n",
    "    kde.fit(data[:, np.newaxis])\n",
    "    x_d = np.linspace(x_min, x_max, n_bins)\n",
    "    log_dens = kde.score_samples(x_d[:, np.newaxis])\n",
    "    return np.exp(log_dens), x_d\n",
    "\n",
    "# Bootstrap KDE\n",
    "def bootstrap_kde(data, n_bootstrap=1000, bandwidth=0.1, n_bins=100, x_min=0, x_max=10):\n",
    "    kde_samples = np.zeros((n_bootstrap, n_bins))\n",
    "    for i in range(n_bootstrap):\n",
    "        resample = np.random.choice(data, size=len(data), replace=True)\n",
    "        kde_samples[i], x_d = compute_kde(resample, bandwidth, n_bins, x_min, x_max)\n",
    "    kde_mean = np.mean(kde_samples, axis=0)\n",
    "    kde_std = np.std(kde_samples, axis=0)\n",
    "    return kde_mean, kde_std, x_d\n",
    "\n",
    "# Normalize the x-axis\n",
    "lattice_constant = 1\n",
    "n_bins = 100\n",
    "bandwidth = 0.1\n",
    "\n",
    "# Concatenate all histograms\n",
    "combined_atom_histogram = np.concatenate(master_atom_histogram)\n",
    "combined_2H_map_histogram = np.concatenate(master_2H_map_histogram)\n",
    "\n",
    "# Apply KDE with bootstrapping to both histograms\n",
    "kde_atom_mean, kde_atom_std, x_d = bootstrap_kde(combined_atom_histogram, n_bootstrap=10, bandwidth=bandwidth, n_bins=n_bins)\n",
    "kde_2H_map_mean, kde_2H_map_std, _ = bootstrap_kde(combined_2H_map_histogram, n_bootstrap=10, bandwidth=bandwidth, n_bins=n_bins)\n",
    "\n",
    "# Compute bin edges for plotting\n",
    "binedges_total = x_d / lattice_constant\n",
    "bin_widths_total = np.diff(binedges_total)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "norm = Normalize(vmin=np.min(binedges_total), vmax=np.max(binedges_total))\n",
    "colors_total = inferno(norm(binedges_total[:-1]))\n",
    "\n",
    "# KDE plot for 2H Map Density with error bands\n",
    "ax.plot(binedges_total, kde_2H_map_mean, color='purple', label='2H Map Density')\n",
    "ax.fill_between(binedges_total, kde_2H_map_mean - kde_2H_map_std, kde_2H_map_mean + kde_2H_map_std, color='purple', alpha=0.3)\n",
    "ax.plot(binedges_total, kde_atom_mean, color='green', label='Se Site Density')\n",
    "ax.fill_between(binedges_total, kde_atom_mean - kde_atom_std, kde_atom_mean + kde_atom_std, color='green', alpha=0.3)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlim(-0.1, 6)\n",
    "ax.set_ylim(0, 0.8)\n",
    "for dist in [0, 1, np.sqrt(2)]:\n",
    "    ax.axvline(x=dist * 3.16, ymin=0, ymax=100, color='k', linestyle='--', linewidth=3)\n",
    "\n",
    "plt.xlabel('Distance (Angstroms)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.cm import ScalarMappable\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute bin edges for plotting\n",
    "binedges_total = x_d / lattice_constant\n",
    "bin_widths_total = np.diff(binedges_total)\n",
    "\n",
    "# Compute histogram for 2H map density\n",
    "hist_2H_map_density_total, _ = np.histogram(combined_2H_map_histogram, bins=binedges_total, density=True)\n",
    "hist_2H_map_density_total /= np.sum(hist_2H_map_density_total * bin_widths_total)  # Normalize\n",
    "\n",
    "# Calculate bin midpoints for bar plot\n",
    "bin_midpoints = binedges_total[:-1] + bin_widths_total / 2\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "\n",
    "norm = Normalize(vmin=np.min(binedges_total), vmax=np.max(binedges_total))\n",
    "colors_total = inferno(norm(binedges_total[:-1]))\n",
    "\n",
    "# Bar plot for 2H map density\n",
    "ax.bar(bin_midpoints, hist_2H_map_density_total, width=bin_widths_total, color=colors_total, edgecolor='k', alpha=0.9)\n",
    "\n",
    "# KDE plot for 2H Map Density with error bands\n",
    "ax.plot(binedges_total, kde_2H_map_mean, color='black', linewidth=6)  # Black edge\n",
    "ax.plot(binedges_total, kde_2H_map_mean, color='purple', label='2H distance map', linewidth=4)  # Main line\n",
    "ax.fill_between(binedges_total, kde_2H_map_mean - kde_2H_map_std, kde_2H_map_mean + kde_2H_map_std, color='purple', alpha=0.4)\n",
    "\n",
    "ax.plot(binedges_total, kde_atom_mean, color='black', linewidth=6)  # Black edge\n",
    "ax.plot(binedges_total, kde_atom_mean, color='green', label='Se-site distribution', linewidth=4)\n",
    "ax.fill_between(binedges_total, kde_atom_mean - kde_atom_std, kde_atom_mean + kde_atom_std, color='green', alpha=0.4)\n",
    "\n",
    "stackings = [r'2H', r'1T$^\\prime$', r'1T']\n",
    "for dist, stack in zip([0, 1, np.sqrt(2)], stackings):\n",
    "    ax.axvline(x=dist * 3.16, ymin=0, ymax=100, color='k', linestyle='-', linewidth=3, alpha = 0.8)\n",
    "    ax.text(dist * 3.16 + 0.1, 0.5, stack+' stacking', fontsize=18, va='bottom', color='k')\n",
    "\n",
    "\n",
    "\n",
    "cmap = inferno\n",
    "sm = ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "\n",
    "# Define a custom patch for the legend\n",
    "legend_patch = mpatches.Patch(color='purple', label='2H distance map')  # Placeholder, not gradient\n",
    "# Add the custom patch to the legend\n",
    "ax.legend(handles=[legend_patch, mpatches.Patch(color=cmap(norm(0.5)), label='2H distance map gradient')], loc='upper left')\n",
    "\n",
    "\n",
    "ax.set_xlabel(f'Distance ($\\AA$)')\n",
    "ax.set_xlim(-0.1, 6)\n",
    "\n",
    "ax.set_ylabel('Normalized Density (a.u.)')\n",
    "ax.set_ylim(0, 0.8)\n",
    "ax.set_yticks([])\n",
    "\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean((kde_atom_mean - kde_2H_map_mean) ** 2)\n",
    "\n",
    "print(f'Mean Squared Error (MSE) between the two distributions: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute bin edges for plotting\n",
    "binedges_total = x_d / lattice_constant\n",
    "bin_widths_total = np.diff(binedges_total)\n",
    "\n",
    "# Compute histogram for 2H map density\n",
    "hist_2H_map_density_total, _ = np.histogram(combined_2H_map_histogram, bins=binedges_total, density=True)\n",
    "hist_2H_map_density_total /= np.sum(hist_2H_map_density_total * bin_widths_total)  # Normalize\n",
    "\n",
    "# Calculate bin midpoints for bar plot\n",
    "bin_midpoints = binedges_total[:-1] + bin_widths_total / 2\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(11, 6), dpi=300)\n",
    "\n",
    "norm = Normalize(vmin=np.min(binedges_total), vmax=np.max(binedges_total))\n",
    "colors_total = inferno(norm(binedges_total[:-1]))\n",
    "\n",
    "# Bar plot for 2H map density\n",
    "bar_container = ax.bar(bin_midpoints, hist_2H_map_density_total, width=bin_widths_total, color=colors_total, edgecolor='k', alpha=0.9)\n",
    "\n",
    "# KDE plot for 2H Map Density with error bands\n",
    "line1, = ax.plot(binedges_total, kde_2H_map_mean, color='black', linewidth=6)  # Black edge\n",
    "line2, = ax.plot(binedges_total, kde_2H_map_mean, color='purple', label='2H distance map KDE', linewidth=4)  # Main line\n",
    "ax.fill_between(binedges_total, kde_2H_map_mean - kde_2H_map_std, kde_2H_map_mean + kde_2H_map_std, color='purple', alpha=0.4)\n",
    "\n",
    "line3, = ax.plot(binedges_total, kde_atom_mean, color='black', linewidth=6)  # Black edge\n",
    "line4, = ax.plot(binedges_total, kde_atom_mean, color='green', label='Se-site distribution KDE', linewidth=4)\n",
    "ax.fill_between(binedges_total, kde_atom_mean - kde_atom_std, kde_atom_mean + kde_atom_std, color='green', alpha=0.4)\n",
    "\n",
    "stackings = [r'2H', r'1T$^\\prime$', r'1T']\n",
    "delta_d = 0.04\n",
    "for dist, stack in zip([0, 1, np.sqrt(2)], stackings):\n",
    "    lattice_fudge = delta_d * np.sqrt(2)\n",
    "    # fill from dist * 3.16 - lattice_fudge to dist * 3.16 + lattice_fudge\n",
    "    ax.fill_betweenx([0, 1], dist * 3.16 - lattice_fudge, dist * 3.16 + lattice_fudge, color='k', alpha=0.4)\n",
    "    ax.axvline(x=dist * 3.16, ymin=0, ymax=100, color='k', linestyle='-', linewidth=3, alpha = 0.8)\n",
    "\n",
    "# Add the colorbar to represent the gradient\n",
    "sm = ScalarMappable(cmap=inferno, norm=norm)\n",
    "sm.set_array([])\n",
    "plt.colorbar(sm, ax=ax, orientation='vertical', label='Distance (Angstroms)')\n",
    "\n",
    "# Define a custom patch for the legend\n",
    "legend_patch = mpatches.Patch(color='purple', label='2H distance map')\n",
    "\n",
    "# Update the legend to include all necessary handles\n",
    "ax.legend(handles=[legend_patch, line2, line4], loc='upper left', bbox_to_anchor=(0.01, 1), fontsize=16)\n",
    "\n",
    "ax.set_xlabel(f'Distance ($\\AA$)')\n",
    "ax.set_xlim(-0.1, 6)\n",
    "\n",
    "ax.set_ylabel('Normalized Density (a.u.)')\n",
    "ax.set_ylim(0, 0.8)\n",
    "ax.set_yticks([])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
