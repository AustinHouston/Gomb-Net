{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gomb - Net\n",
    "### Performance test on the Graphene dataset\n",
    "### And analysis of Graphene experimental data\n",
    "\n",
    "Austin Houston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![OpenInColab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "    https://colab.research.google.com/github/ahoust17/Gomb-Net/blob/main/Eval_Graphene_model.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib import cm\n",
    "\n",
    "# colab interactive plots and drive\n",
    "drive = False\n",
    "if 'google.colab' in sys.modules:\n",
    "    from  google.colab import drive \n",
    "    from google.colab import output\n",
    "    drive.mount('/content/drive')\n",
    "    output.enable_custom_widget_manager()\n",
    "    drive = True\n",
    "else:\n",
    "    %matplotlib widget\n",
    "\n",
    "# other imports\n",
    "from scipy.ndimage import label, center_of_mass, gaussian_filter, zoom, uniform_filter\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.stats import norm, gaussian_kde\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.feature import blob_log\n",
    "\n",
    "# for cropping function\n",
    "if drive:\n",
    "    print('installing DataGenSTEM')\n",
    "    !pip install ase\n",
    "    !git clone https://github.com/ahoust17/DataGenSTEM.git\n",
    "    sys.path.append('./DataGenSTEM/DataGenSTEM')\n",
    "    import data_generator as dg\n",
    "\n",
    "# for Gomb-Net\n",
    "if drive:\n",
    "    print('installing Gomb-Net')\n",
    "    !git clone https://github.com/ahoust17/Gomb-Net.git\n",
    "    sys.path.append('./Gomb-Net/GombNet')    \n",
    "from GombNet.networks import *\n",
    "from GombNet.loss_func import GombinatorialLoss\n",
    "from GombNet.utils import *\n",
    "\n",
    "import torch\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set mps, just for my computer\n",
    "# device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, you need to add the following shared drive to your google drive:\n",
    "*** WARNING: it is a big file.  Check before you download ***\n",
    "\n",
    "\n",
    "https://drive.google.com/drive/folders/1tDF283xry5op3t594oBUlcNLKbjRTV7C?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following cell after the download is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be someting like 'content/drive/My Drive/Gomb-Net files'\n",
    "if drive:\n",
    "    shared_folder = 'drive/My Drive/Gomb-Net files'\n",
    "else:\n",
    "    shared_folder = '/Users/austin/Desktop/Gomb-Net aux files'\n",
    "\n",
    "print('available files & directories:')\n",
    "!ls '{shared_folder}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, on to running the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "images_dir = str(shared_folder + '/Graphene_dataset/images')\n",
    "labels_dir = str(shared_folder + '/Graphene_dataset/labels')\n",
    "train_loader, val_loader, test_loader = get_dataloaders(images_dir, labels_dir, batch_size = 1, val_split=0.2, test_split=0.1, seed = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = 3\n",
    "test = test_loader.dataset[test_iter][0].unsqueeze(0)\n",
    "gt = test_loader.dataset[test_iter][1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "ax[0].imshow(test[0, 0].cpu().numpy(), cmap='gray')\n",
    "ax[0].set_title('Input')\n",
    "\n",
    "titles = ['L1: C', 'L2: C']\n",
    "for i in range(2):\n",
    "    ax[i+1].imshow(gt[i].cpu().numpy(), cmap='gray')\n",
    "    ax[i+1].set_title(titles[i])\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's look at the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "input_channels = 1\n",
    "num_classes = 2\n",
    "num_filters = [32, 64, 128, 256]\n",
    "\n",
    "model = TwoLeggedUnet(input_channels, num_classes, num_filters, dropout = 0.2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss = GombinatorialLoss(group_size = num_classes//2, loss = 'Dice', epsilon=1e-6, class_weights = None, alpha=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of trainable parameters\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "num_trainable_params = count_trainable_parameters(model)\n",
    "print(f\"Number of trainable parameters: {num_trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the training history for the pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = np.load(str(shared_folder + '/Pretrained_models/Graphene_model_loss_history.npz'))\n",
    "train_loss = loss_history['train_loss_history']\n",
    "val_loss = loss_history['val_loss_history']\n",
    "\n",
    "plt.figure(figsize = (6,4))\n",
    "plt.plot(train_loss, label='training', color = '#1f77b4')\n",
    "plt.plot(val_loss, label='validation', color = '#d62728')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xlim(0,30)\n",
    "plt.legend(title='Losses')\n",
    "plt.tight_layout()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load in the pretrained weights onto our model 'skeleton'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = str(shared_folder + '/Pretrained_models/Graphene_model.pth')\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = 0\n",
    "\n",
    "test = test_loader.dataset[test_iter][0].unsqueeze(0)\n",
    "gt = test_loader.dataset[test_iter][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prediction\n",
    "with torch.no_grad():\n",
    "    #test.to(device)\n",
    "    probability = model(test)\n",
    "    prediction = F.sigmoid(probability)#>0.50\n",
    "probability = probability.squeeze().cpu().numpy() \n",
    "prediction = prediction.squeeze().cpu().numpy()\n",
    "\n",
    "threshold = threshold_otsu(prediction)\n",
    "prediction = (prediction > threshold).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(test.squeeze().cpu().numpy(), cmap='gray')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "num_classes\n",
    "fig, axs = plt.subplots(3,num_classes,dpi = 300, sharex=True, sharey=True)\n",
    "\n",
    "for i in range(num_classes):\n",
    "    axs[0,i].imshow(gt[i], cmap='gray')\n",
    "\n",
    "for i in range(num_classes):\n",
    "    axs[1,i].imshow(prediction[i], cmap='gray')\n",
    "\n",
    "for i in range(num_classes)[:1]:\n",
    "    axs[2,i].imshow(probability[i], cmap='plasma')\n",
    "for i in range(num_classes)[1:]:\n",
    "    axs[2,i].imshow(probability[i], cmap='viridis')\n",
    "\n",
    "\n",
    "for ax in axs.ravel():\n",
    "    ax.axis('off')\n",
    "\n",
    "axs[0,0].set_ylabel('GrounTruth')\n",
    "axs[1,0].set_ylabel('Prediction')\n",
    "axs[2,0].set_ylabel('Probability')\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to blob-finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = 0\n",
    "# try a regular blob finder for comparison\n",
    "blob_im = test.squeeze().cpu().numpy()\n",
    "\n",
    "blobs = blob_log(blob_im, min_sigma=1, max_sigma=20, num_sigma=5, threshold=0.1)\n",
    "blobs_com = [center_of_mass(blob_im, blobs[i, 0], blobs[i, 1]) for i in range(blobs.shape[0])]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(blob_im, cmap='gray')\n",
    "plt.scatter(blobs[:, 1], blobs[:, 0], c='r', s=20)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is marked out, because it takes a few minutes to run.  Just measuring network performance metrics across the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwa_total = 0\n",
    "# dice_total = 0\n",
    "# IoU_total = 0\n",
    "# \n",
    "# def iou(pred, gt):\n",
    "#     intersection = np.logical_and(pred, gt).sum()\n",
    "#     union = np.logical_or(pred, gt).sum()\n",
    "#     return intersection / union\n",
    "# \n",
    "# def dice_coefficient(pred, gt):\n",
    "#     intersection = np.logical_and(pred, gt).sum()\n",
    "#     return 2 * intersection / (pred.sum() + gt.sum())\n",
    "# \n",
    "# # Calculate the accuracy\n",
    "# for i in range(len(test_loader)):\n",
    "#     test = test_loader.dataset[i][0].unsqueeze(0)\n",
    "#     gt = test_loader.dataset[i][1].numpy()  # Convert to numpy array\n",
    "#     \n",
    "#     # Switch ground truth layers\n",
    "#     gt_switched = np.flip(gt, axis=0)\n",
    "#     \n",
    "#     with torch.no_grad():\n",
    "#         probability = model(test)\n",
    "#         prediction = torch.sigmoid(probability)  # Use torch.sigmoid instead of F.sigmoid (deprecated)\n",
    "#     \n",
    "#     probability = probability.squeeze().cpu().numpy()\n",
    "#     prediction = prediction.squeeze().cpu().numpy()\n",
    "# \n",
    "#     threshold = threshold_otsu(prediction)\n",
    "#     prediction = (prediction > threshold).astype(float)\n",
    "#     \n",
    "#     # Calculate metrics for original and switched ground truths\n",
    "#     pwa_original = np.sum(prediction == gt) / np.prod(gt.shape)\n",
    "#     pwa_switched = np.sum(prediction == gt_switched) / np.prod(gt_switched.shape)\n",
    "#     \n",
    "#     dice_original = dice_coefficient(prediction, gt)\n",
    "#     dice_switched = dice_coefficient(prediction, gt_switched)\n",
    "#     \n",
    "#     iou_original = iou(prediction, gt)\n",
    "#     iou_switched = iou(prediction, gt_switched)\n",
    "#     \n",
    "#     # Take the highest value for each metric\n",
    "#     pwa_total += max(pwa_original, pwa_switched)\n",
    "#     dice_total += max(dice_original, dice_switched)\n",
    "#     IoU_total += max(iou_original, iou_switched)\n",
    "# \n",
    "# # Calculate the average for each metric\n",
    "# pwa_total /= len(test_loader)\n",
    "# dice_total /= len(test_loader)\n",
    "# IoU_total /= len(test_loader)\n",
    "# \n",
    "# print(f\"Pixel-wise Accuracy: {pwa_total}\")\n",
    "# print(f\"Mean Dice Coefficient: {dice_total}\")\n",
    "# print(f\"Mean IoU: {IoU_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, on Experimental data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data = np.load(str(shared_folder + '/Experimental_datasets/moire.npz'))\n",
    "im_array = exp_data['im_array']\n",
    "pixel_size = exp_data['pixel_size']\n",
    "\n",
    "\n",
    "# im_array = gaussian_filter(im_array, sigma=1)\n",
    "im_array = im_array - np.min(im_array)\n",
    "im_array = im_array / np.max(im_array)\n",
    "# zoom_factor = 0.7\n",
    "# \n",
    "# im_array = dg.resize_image(im_array, zoom_factor * 512)\n",
    "\n",
    "print(f\"Pixel size: {pixel_size.astype(float)} m/pix\")\n",
    "plt.figure()\n",
    "plt.imshow(im_array, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = dg.shotgun_crop(im_array, crop_size=256, n_crops = 5, roi = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 5, figsize=(10, 10), dpi = 300)\n",
    "masks = np.zeros((5, 2, 256, 256))\n",
    "\n",
    "for i, im in enumerate(images):\n",
    "    # make nn prediction\n",
    "    im = im.astype(np.float32)\n",
    "    im = torch.tensor(im).unsqueeze(0).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        probability = model(im)\n",
    "        prediction = torch.sigmoid(probability)\n",
    "    probability = probability.squeeze().cpu().numpy()\n",
    "    prediction = prediction.squeeze().cpu().numpy()\n",
    "    threshold = threshold_otsu(prediction)\n",
    "    prediction = (prediction > threshold).astype(float)\n",
    "    masks[i] = prediction\n",
    "\n",
    "\n",
    "    # plot\n",
    "    ax[i, 0].imshow(im.squeeze().numpy(), cmap='gray')\n",
    "    ax[i, 1].imshow(prediction[0], cmap='gray')\n",
    "    ax[i, 2].imshow(prediction[1], cmap='gray')\n",
    "    ax[i, 3].imshow(probability[0], cmap='plasma')\n",
    "    ax[i, 4].imshow(probability[1], cmap='viridis')\n",
    "\n",
    "for a in ax.ravel():\n",
    "    a.axis('off')\n",
    "fig.tight_layout()\n",
    "\n",
    "#plt.savefig('moire_segmentation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_factor = 4\n",
    "zoom_order = 3\n",
    "i = 0\n",
    "dist_hist = []\n",
    "colors = ['#d62728','#1f77b4']\n",
    "for image, mask in zip(images, masks):\n",
    "    # fig, ax = plt.subplots(1, 2, dpi=300, figsize=(5, 5), subplot_kw={'aspect': 'equal'})\n",
    "    for layer, color, a in zip(mask, colors, ax):\n",
    "        # resize layer\n",
    "        layer = zoom(layer, resize_factor, order=zoom_order)\n",
    "        layer = gaussian_filter(layer, sigma=1)\n",
    "        threshold = threshold_otsu(layer)\n",
    "        layer = (layer > threshold).astype(float)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(layer, cmap='gray')\n",
    "\n",
    "        labeled_array, num_features = label(layer)\n",
    "        centroids = center_of_mass(layer, labeled_array, range(1, num_features + 1))\n",
    "        centroids = np.array(centroids)\n",
    "\n",
    "        # Calculate the distance between the centroids\n",
    "        tree = KDTree(centroids)\n",
    "        distances, indices = tree.query(centroids, k=3)\n",
    "        nearest_distances = distances[:, 1:] * float(pixel_size) # angstroms\n",
    "        dist_hist.append(nearest_distances.flatten())\n",
    "\n",
    "    #     a.scatter(centroids[:, 1], centroids[:, 0], s=38, c=color, edgecolors='k', linewidths=0.5)\n",
    "    #     a.axis('off')\n",
    "    #     a.set_xlim(0, layer.shape[1])\n",
    "    #     a.set_ylim(layer.shape[0], 0)  # Flip the y-axis to match image coordinates\n",
    "    #     \n",
    "    # plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    #plt.savefig(f'atoms_{i}.png', transparent=True, bbox_inches='tight', pad_inches=0)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dist_hist a 1D array\n",
    "dist_hist = np.concatenate(dist_hist) * 1e10 / resize_factor\n",
    "\n",
    "avg_dist = np.mean(dist_hist)\n",
    "\n",
    "plt.figure(figsize = (8,8), dpi=300)\n",
    "plt.hist(dist_hist, bins=100, color='gray')\n",
    "\n",
    "plt.vlines(1.42, 0, 1000, color='k', label='C-C bond 1.42 Å')\n",
    "plt.vlines(avg_dist, 0, 1000, color='k', linestyle = '--', label=f'Avg. bond {avg_dist:.2f} Å')\n",
    "plt.xlabel('Distance (Å)', fontsize=24)\n",
    "plt.xlim(0.8,2)\n",
    "plt.ylim(0,850)\n",
    "plt.legend(loc='upper right', fontsize=24)\n",
    "# set xtick fontsize\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks([])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average distance: {avg_dist:.2f} Å\")\n",
    "\n",
    "# FWHM\n",
    "half_max = np.max(np.histogram(dist_hist, bins=100)[0]) / 2\n",
    "hist, bin_edges = np.histogram(dist_hist, bins=100)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "fwhm = bin_centers[hist > half_max]\n",
    "fwhm = fwhm[-1] - fwhm[0]\n",
    "print(f\"FWHM: {fwhm:.2f} Å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is some old code not used in the paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, crop_size, stride):\n",
    "    crops = []\n",
    "    coords = []\n",
    "    for i in range(0, image.shape[0] - crop_size + 1, stride):\n",
    "        for j in range(0, image.shape[1] - crop_size + 1, stride):\n",
    "            crop = image[i:i+crop_size, j:j+crop_size]\n",
    "            crops.append(crop)\n",
    "            coords.append((i, j))\n",
    "    return np.array(crops), coords\n",
    "\n",
    "def reconstruct_image(crops, coords, image_shape, crop_size, stride):\n",
    "    reconstructed = np.zeros(image_shape, dtype=np.float32)\n",
    "    count_map = np.zeros(image_shape, dtype=np.float32)\n",
    "\n",
    "    for crop, (i, j) in zip(crops, coords):\n",
    "        reconstructed[i:i+crop_size, j:j+crop_size] += crop\n",
    "        count_map[i:i+crop_size, j:j+crop_size] += 1\n",
    "\n",
    "    return reconstructed / count_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 256\n",
    "stride = 16 \n",
    "\n",
    "# load data\n",
    "exp_data = np.load('./Exp_data/moire.npz')\n",
    "im_array = exp_data['im_array']\n",
    "pixel_size = exp_data['pixel_size']\n",
    "pixel_size = float(pixel_size) * 1e10  # Convert to angstroms\n",
    "\n",
    "im_array = im_array - np.min(im_array)\n",
    "im_array = im_array / np.max(im_array)\n",
    "\n",
    "# pad input image\n",
    "im_array = np.pad(im_array, ((crop_size//2, crop_size//2), (crop_size//2, crop_size//2)), mode='wrap')\n",
    "\n",
    "crops, coords = crop_image(im_array, crop_size, stride)\n",
    "crops = crops.astype(np.float32)\n",
    "crops = torch.tensor(crops).unsqueeze(1)\n",
    "\n",
    "# make prediction\n",
    "with torch.no_grad():\n",
    "    probability = model(crops)\n",
    "probability = probability.numpy()\n",
    "\n",
    "prob_A = probability[:, 0, :, :]\n",
    "prob_B = probability[:, 1, :, :]\n",
    "\n",
    "map_A = reconstruct_image(prob_A, coords, im_array.shape, crop_size, stride)\n",
    "map_B = reconstruct_image(prob_B, coords, im_array.shape, crop_size, stride)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-pad the image and maps\n",
    "im_array = im_array[crop_size//2:-crop_size//2, crop_size//2:-crop_size//2]\n",
    "map_A = map_A[crop_size//2:-crop_size//2, crop_size//2:-crop_size//2]\n",
    "map_B = map_B[crop_size//2:-crop_size//2, crop_size//2:-crop_size//2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3, figsize=(15,5), dpi = 300)\n",
    "ax[0].imshow(im_array, cmap='gray')\n",
    "ax[1].imshow(map_A, cmap='plasma')\n",
    "ax[2].imshow(map_B, cmap='viridis')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms of the maps\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.hist(map_A.flatten(), bins=100, color='purple', alpha=0.5, label='Class A');\n",
    "plt.hist(map_B.flatten(), bins=100, color='green', alpha=0.5, label='Class B');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_hist = []\n",
    "colors = ['#d62728','#1f77b4']\n",
    "for layer, color in zip([map_A, map_B], colors):\n",
    "    # crop off the outside 16 pixels\n",
    "\n",
    "\n",
    "    layer = (layer > 0).astype(float)\n",
    "\n",
    "    fig, ax = plt.subplots(1,3,sharex=True, sharey = True, dpi=300, figsize=(14,8))\n",
    "    ax[0].imshow(layer, cmap='gray')\n",
    "\n",
    "    labeled_array, num_features = label(layer)\n",
    "    centroids = center_of_mass(layer, labeled_array, range(1, num_features + 1))\n",
    "    centroids = np.array(centroids)\n",
    "\n",
    "    # Calculate the distance between the centroids\n",
    "    tree = KDTree(centroids)\n",
    "    distances, indices = tree.query(centroids, k=3)\n",
    "    nearest_distances = distances[:, 1:] * float(pixel_size) # angstroms\n",
    "    dist_hist.append(nearest_distances.flatten())\n",
    "\n",
    "    ax[1].imshow(np.zeros_like(layer)+1,vmax=1,vmin=0, cmap='gray')\n",
    "    ax[1].scatter(centroids[:, 1], centroids[:, 0], s=30, c=color, edgecolors='k', linewidths=0.5)\n",
    "    ax[1].set_xlim(0, layer.shape[1])\n",
    "    ax[1].set_ylim(layer.shape[0], 0)  # Flip the y-axis to match image coordinates\n",
    "\n",
    "    for a in ax:\n",
    "        a.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_size * len(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_array_A, num_features_A = label(map_A > 0)\n",
    "centroids_A = center_of_mass(map_A, labeled_array_A, range(1, num_features_A + 1))\n",
    "centroids_A = np.array(centroids_A)\n",
    "\n",
    "labeled_array_B, num_features_B = label(map_B > 0)\n",
    "centroids_B = center_of_mass(map_B, labeled_array_B, range(1, num_features_B + 1))\n",
    "centroids_B = np.array(centroids_B)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
    "ax[0].imshow(im_array, cmap='gray')\n",
    "ax[1].imshow(im_array, cmap='gray')\n",
    "ax[0].scatter(centroids_A[:, 1], centroids_A[:, 0], s=60, c='purple', edgecolors='k', linewidths=0.5, alpha = 0.8)\n",
    "ax[1].scatter(centroids_B[:, 1], centroids_B[:, 0], s=60, c='green', edgecolors='k', linewidths=0.5, alpha = 0.8)\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Strain map of both layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create KDTree and find the distances to the nearest 3 centroids\n",
    "tree_A = KDTree(centroids_A)\n",
    "distances_A, _ = tree_A.query(centroids_A, k=4)  # k=4 because the closest point will be itself\n",
    "\n",
    "tree_B = KDTree(centroids_B)\n",
    "distances_B, _ = tree_B.query(centroids_B, k=4)\n",
    "\n",
    "# Calculate the average bond distance for the nearest 3 neighbors (excluding itself)\n",
    "average_distance_A = np.mean(distances_A[:, 1:4], axis=1)\n",
    "average_distance_B = np.mean(distances_B[:, 1:4], axis=1)\n",
    "\n",
    "# Calculate strain as deviation from the average bond distance\n",
    "strain_A = distances_A[:, 1:4] - average_distance_A[:, None]\n",
    "strain_B = distances_B[:, 1:4] - average_distance_B[:, None]\n",
    "\n",
    "# Flatten the strain arrays for plotting\n",
    "strain_A_flat = strain_A.flatten()\n",
    "strain_B_flat = strain_B.flatten()\n",
    "\n",
    "# Plot the strain maps\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
    "ax[0].imshow(im_array, cmap='gray')\n",
    "ax[1].imshow(im_array, cmap='gray')\n",
    "\n",
    "# Scatter plot of strain with color representing the strain magnitude\n",
    "sc_A = ax[0].scatter(centroids_A[:, 1], centroids_A[:, 0], c=np.mean(strain_A, axis=1), cmap='viridis', s=60, edgecolors='k', linewidths=0.5)\n",
    "sc_B = ax[1].scatter(centroids_B[:, 1], centroids_B[:, 0], c=np.mean(strain_B, axis=1), cmap='viridis', s=60, edgecolors='k', linewidths=0.5)\n",
    "\n",
    "# Colorbar\n",
    "cbar_A = plt.colorbar(sc_A, ax=ax[0], fraction=0.046, pad=0.04)\n",
    "cbar_A.set_label('Strain Magnitude')\n",
    "\n",
    "cbar_B = plt.colorbar(sc_B, ax=ax[1], fraction=0.046, pad=0.04)\n",
    "cbar_B.set_label('Strain Magnitude')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create KDTree and find the distances to the nearest 3 centroids\n",
    "tree_A = KDTree(centroids_A)\n",
    "distances_A, _ = tree_A.query(centroids_A, k=4)  # k=4 because the closest point will be itself\n",
    "\n",
    "tree_B = KDTree(centroids_B)\n",
    "distances_B, _ = tree_B.query(centroids_B, k=4)\n",
    "\n",
    "# Calculate the average bond distance for the nearest 3 neighbors (excluding itself)\n",
    "average_distance_A = np.mean(distances_A[:, 1:4], axis=1)\n",
    "average_distance_B = np.mean(distances_B[:, 1:4], axis=1)\n",
    "\n",
    "# Calculate strain as deviation from the average bond distance\n",
    "strain_A = distances_A[:, 1:4] - average_distance_A[:, None]\n",
    "strain_B = distances_B[:, 1:4] - average_distance_B[:, None]\n",
    "\n",
    "# Average strain per centroid\n",
    "average_strain_A = np.mean(strain_A, axis=1)\n",
    "average_strain_B = np.mean(strain_B, axis=1)\n",
    "\n",
    "# Create a grid for interpolation\n",
    "grid_x, grid_y = np.mgrid[0:512, 0:512]\n",
    "\n",
    "# Interpolate strain values over a 512x512 grid\n",
    "strain_map_A = griddata(centroids_A, average_strain_A, (grid_x, grid_y), method='cubic', fill_value=0)\n",
    "strain_map_B = griddata(centroids_B, average_strain_B, (grid_x, grid_y), method='cubic', fill_value=0)\n",
    "\n",
    "# Plot the interpolated strain maps\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
    "cax_A = ax[0].imshow(strain_map_A, cmap='viridis', origin='lower')\n",
    "cax_B = ax[1].imshow(strain_map_B, cmap='viridis', origin='lower')\n",
    "\n",
    "ax[0].set_title('Strain Map A')\n",
    "ax[1].set_title('Strain Map B')\n",
    "\n",
    "# Colorbars\n",
    "cbar_A = plt.colorbar(cax_A, ax=ax[0], fraction=0.046, pad=0.04)\n",
    "cbar_A.set_label('Strain Magnitude')\n",
    "\n",
    "cbar_B = plt.colorbar(cax_B, ax=ax[1], fraction=0.046, pad=0.04)\n",
    "cbar_B.set_label('Strain Magnitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute correlation between the two strain maps\n",
    "correlation = np.corrcoef(strain_map_A.flatten(), strain_map_B.flatten())[0, 1]\n",
    "print(f'Correlation between the strain maps: {correlation:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute local correlation\n",
    "def local_correlation(x, y, window_size):\n",
    "    x_mean = uniform_filter(x, window_size)\n",
    "    y_mean = uniform_filter(y, window_size)\n",
    "    x2_mean = uniform_filter(x**2, window_size)\n",
    "    y2_mean = uniform_filter(y**2, window_size)\n",
    "    xy_mean = uniform_filter(x * y, window_size)\n",
    "    \n",
    "    covariance = xy_mean - x_mean * y_mean\n",
    "    variance_x = x2_mean - x_mean**2\n",
    "    variance_y = y2_mean - y_mean**2\n",
    "    \n",
    "    correlation_map = covariance / np.sqrt(variance_x * variance_y)\n",
    "    return correlation_map\n",
    "\n",
    "# Compute local correlation map\n",
    "window_size = 21  # You can adjust the window size\n",
    "correlation_map = local_correlation(strain_map_A, strain_map_B, window_size)\n",
    "\n",
    "# Plot the correlation map\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(np.abs(correlation_map), cmap='coolwarm', origin='lower')\n",
    "plt.colorbar(label='Local Correlation')\n",
    "plt.title('Local Correlation Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(correlation_map.ravel());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lattice_param = 1.39 # angstroms\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find the nearest neighbors in layer A for each atom in layer B\n",
    "tree_B = KDTree(centroids_B)\n",
    "distances, indices = tree_B.query(centroids_A)\n",
    "\n",
    "# Calculate the displacement vectors\n",
    "order_parameter_vectors = centroids_B[indices] - centroids_A\n",
    "order_parameters = np.linalg.norm(order_parameter_vectors, axis=1)  # just the magnitude\n",
    "order_parameters = order_parameters * pixel_size\n",
    "angles = np.arctan2(order_parameter_vectors[:, 1], order_parameter_vectors[:, 0])\n",
    "norm_angles = plt.Normalize(vmin=-np.pi, vmax=np.pi)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "# Normalize vectors for quiver plot\n",
    "u = order_parameter_vectors[:, 0] / order_parameters\n",
    "v = order_parameter_vectors[:, 1] / order_parameters\n",
    "plt.quiver(centroids_A[:, 0], centroids_A[:, 1], u, v, order_parameters, cmap='viridis', scale=1, scale_units='xy', angles='xy')\n",
    "\n",
    "plt.scatter(centroids_A[:, 0], centroids_A[:, 1], s=60, c='purple', edgecolors='k', linewidths=0.5, alpha=0.8)\n",
    "plt.scatter(centroids_B[:, 0], centroids_B[:, 1], s=60, c='green', edgecolors='k', linewidths=0.5, alpha=0.8)\n",
    "\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.title('Order parameter u map - graphene moire')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create empty numpy arrays for 512x512 grid\n",
    "image_size = 512\n",
    "angle_image = np.zeros((image_size, image_size))\n",
    "magnitude_image = np.zeros((image_size, image_size))\n",
    "\n",
    "# Build a KDTree for the centroids in layer B\n",
    "tree_B = KDTree(centroids_B)\n",
    "\n",
    "# Assign values to each pixel based on the closest order parameter vector\n",
    "for x in range(image_size):\n",
    "    for y in range(image_size):\n",
    "        _, index = tree_B.query([x, y])\n",
    "        angle_image[y, x] = angles[index]\n",
    "        magnitude_image[y, x] = magnitudes[index]\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the angle image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(angle_image, cmap='twilight', origin='lower')\n",
    "plt.colorbar(label='Direction (radians)')\n",
    "plt.title('Angle Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the magnitude image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(magnitude_image, cmap='viridis', origin='lower')\n",
    "plt.colorbar(label='Magnitude')\n",
    "plt.title('Magnitude Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
